[2023-08-24 13:15:22,218]160numexpr.utils-INFO-NumExpr defaulting to 6 threads.
[2023-08-24 13:15:24,723]361flask_cors.core-WARNING-Unknown option passed to Flask-CORS: support_credentials
[2023-08-24 13:15:24,724]361flask_cors.core-WARNING-Unknown option passed to Flask-CORS: support_credentials
[2023-08-24 13:15:31,941]16chromadb.telemetry.posthog-INFO-Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
[2023-08-24 13:15:35,998]187werkzeug-INFO-[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.2:5000
[2023-08-24 13:15:35,998]187werkzeug-INFO-[33mPress CTRL+C to quit[0m
[2023-08-24 13:16:09,234]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:16:09] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:16:58,656]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:16:58] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:17:44,327]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:17:44] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:19:02,384]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:19:02] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:19:27,831]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:19:27] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:20:18,529]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:20:18] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:20:57,160]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4310 tokens (4054 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:20:57,160]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4310 tokens (4054 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:20:57,281]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:20:57] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:21:27,015]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:21:27,015]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:21:27,018]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:21:27] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:36:38,325]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:36:38,326]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:36:38,328]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:36:38] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:36:44,723]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:36:44,724]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:36:44,726]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:36:44] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:36:49,371]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:36:49,372]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:36:49,375]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:36:49] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:39:10,350]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:39:10,350]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4315 tokens (4059 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:39:10,353]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:39:10] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:39:27,766]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:39:27] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:40:14,305]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4591 tokens (4335 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:40:14,305]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4591 tokens (4335 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:40:14,308]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:40:14] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:40:18,423]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4591 tokens (4335 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:40:18,423]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4591 tokens (4335 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:40:18,425]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:40:18] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:41:01,621]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:41:01,621]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:41:01,624]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:41:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:42:21,232]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:42:21,232]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:42:21,235]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:42:21] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:42:23,586]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:42:23,586]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:42:23,588]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:42:23] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:42:25,303]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:42:25,303]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    memory = ConversationBufferMemory(input_key="question", memory_key="history")
               ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:42:25,305]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:42:25] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:44:00,776]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:44:00] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
[2023-08-24 13:44:53,957]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:44:53,957]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:44:53,971]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:44:53] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:44:56,085]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:44:56,086]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:44:56,088]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:44:56] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:44:57,954]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:44:57,954]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4176 tokens (3920 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:44:57,957]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:44:57] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:45:15,587]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4587 tokens (4331 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:45:15,589]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4587 tokens (4331 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:45:15,591]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:45:15] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:45:17,830]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4587 tokens (4331 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:45:17,830]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4587 tokens (4331 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:45:17,833]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:45:17] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:45:59,928]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4593 tokens (4337 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:45:59,928]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4593 tokens (4337 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:45:59,931]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:45:59] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:46:31,380]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4174 tokens (3918 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:46:31,382]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4174 tokens (3918 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:46:31,384]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:46:31] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:46:52,507]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4178 tokens (3922 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:46:52,507]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4178 tokens (3922 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:46:52,510]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:46:52] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:47:45,172]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:47:45] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:49:51,546]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:49:51] "POST /ask HTTP/1.1" 200 -
[2023-08-24 13:50:08,895]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:50:08,895]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    query =qa(user_query)
           ^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:50:08,898]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:50:08] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:52:06,625]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:52:06,625]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:52:06,628]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:52:06] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:52:09,081]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:52:09,083]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:52:09,085]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:52:09] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:52:11,690]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:52:11,690]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:52:11,693]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:52:11] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:52:13,973]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:52:13,973]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:52:13,975]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:52:13] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:52:17,607]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:52:17,609]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:52:17,611]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:52:17] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:54:12,311]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:54:12,312]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4298 tokens (4042 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:54:12,315]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:54:12] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:54:21,226]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4294 tokens (4038 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:54:21,226]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4294 tokens (4038 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:54:21,229]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:54:21] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:54:24,710]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4294 tokens (4038 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:54:24,711]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4294 tokens (4038 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:54:24,713]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:54:24] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
[2023-08-24 13:54:46,880]67openai-INFO-error_code=None error_message="This model's maximum context length is 4097 tokens, however you requested 4280 tokens (4024 in your prompt; 256 for the completion). Please reduce your prompt; or completion length." error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
[2023-08-24 13:54:46,880]1414app-ERROR-Exception on /ask [POST]
Traceback (most recent call last):
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\app.py", line 74, in ask_question
    result = analyze_and_answer(question, vectordb)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Marvel\eds\modules\analysis.py", line 155, in analyze_and_answer
    """
        
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\retrieval_qa\base.py", line 140, in _call
    answer = self.combine_documents_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 480, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\base.py", line 106, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\combine_documents\stuff.py", line 172, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 256, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 282, in __call__
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\base.py", line 276, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 92, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py", line 102, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 467, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 598, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 504, in _generate_helper
    raise e
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\base.py", line 491, in _generate_helper
    self._generate(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 384, in _generate
    response = completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 116, in completion_with_retry
    return _completion_with_retry(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\langchain\llms\openai.py", line 114, in _completion_with_retry
    return llm.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_resources\abstract\engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "C:\Users\parth.chaturvedi\AppData\Roaming\Python\Python311\site-packages\openai\api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4280 tokens (4024 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.
[2023-08-24 13:54:46,882]187werkzeug-INFO-127.0.0.1 - - [24/Aug/2023 13:54:46] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
